"""
DoÄŸal dil metinleri iÃ§in tokenizer test modÃ¼lÃ¼.

Bu test modÃ¼lÃ¼, projede bulunan tÃ¼m tokenizer tiplerini
farklÄ± doÄŸal dil metinleri Ã¼zerinde test eder.
"""

import unittest
import os
import tempfile
from pathlib import Path
import json
from typing import Dict, Any, List, Type, Optional

from aiquantr_tokenizer.tokenizers.base import BaseTokenizer, TokenizerTrainer


class TestNaturalLanguage(unittest.TestCase):
    """
    FarklÄ± doÄŸal dil metinleri Ã¼zerinde tokenizer'larÄ± test eder.
    """
    
    def setUp(self):
        """
        Test ortamÄ±nÄ± hazÄ±rlar.
        """
        # Gerekli modÃ¼lleri import et
        try:
            # TÃ¼m tokenizer sÄ±nÄ±flarÄ±
            from aiquantr_tokenizer.tokenizers.bpe import BPETokenizer
            from aiquantr_tokenizer.tokenizers.wordpiece import WordPieceTokenizer
            from aiquantr_tokenizer.tokenizers.byte_level import ByteLevelTokenizer
            from aiquantr_tokenizer.tokenizers.unigram import UnigramTokenizer
            from aiquantr_tokenizer.tokenizers.mixed import MixedTokenizer
            from aiquantr_tokenizer.tokenizers.factory import create_tokenizer_from_config, register_tokenizer_type
            
            self.BPETokenizer = BPETokenizer
            self.WordPieceTokenizer = WordPieceTokenizer
            self.ByteLevelTokenizer = ByteLevelTokenizer
            self.UnigramTokenizer = UnigramTokenizer
            self.MixedTokenizer = MixedTokenizer
            self.create_tokenizer_from_config = create_tokenizer_from_config
            self.register_tokenizer_type = register_tokenizer_type
            
            self.all_tokenizer_classes = {
                "BPE": BPETokenizer,
                "WordPiece": WordPieceTokenizer,
                "ByteLevel": ByteLevelTokenizer, 
                "Unigram": UnigramTokenizer,
            }
            
        except ImportError as e:
            self.skipTest(f"Gerekli tokenizer modÃ¼lleri bulunamadÄ±: {e}")
        
        # GeÃ§ici dizin oluÅŸtur
        self.temp_dir = tempfile.TemporaryDirectory()
        self.temp_path = Path(self.temp_dir.name)
        
        # Dil Ã¶rnekleri
        self.language_samples = {
            "turkish": self._get_turkish_sample(),
            "english": self._get_english_sample(),
            "mixed": self._get_mixed_language_sample(),
            "technical": self._get_technical_sample(),
            "social_media": self._get_social_media_sample()
        }
    
    def tearDown(self):
        """
        Test ortamÄ±nÄ± temizler.
        """
        self.temp_dir.cleanup()
    
    def _get_turkish_sample(self):
        """
        TÃ¼rkÃ§e metin Ã¶rneÄŸi oluÅŸturur.
        """
        return """DoÄŸal dil iÅŸleme, bilgisayar biliminin ve yapay zekanÄ±n Ã¶nemli bir alt dalÄ±dÄ±r. 
        Ä°nsan dilinin anlaÅŸÄ±lmasÄ± ve Ã¼retilmesi amacÄ±yla Ã§eÅŸitli algoritmalar ve yÃ¶ntemler iÃ§erir. 
        GÃ¼nÃ¼mÃ¼zde, sesli asistanlar, otomatik Ã§eviri sistemleri, metin Ã¶zetleme araÃ§larÄ± ve duygu analizi uygulamalarÄ± gibi birÃ§ok teknolojik Ã¼rÃ¼nde kullanÄ±lmaktadÄ±r.
        
        Tokenizasyon, doÄŸal dil iÅŸlemede temel bir Ã¶n iÅŸleme adÄ±mÄ±dÄ±r. Metin verilerini daha kÃ¼Ã§Ã¼k birimlere ayÄ±rmayÄ± amaÃ§lar. 
        Bu birimler kelimeler, alt kelimeler veya karakterler olabilir. Tokenizasyon yÃ¶nteminin seÃ§imi, dilin yapÄ±sÄ±na ve Ã§Ã¶zÃ¼lmeye Ã§alÄ±ÅŸÄ±lan probleme baÄŸlÄ±dÄ±r.
        
        TÃ¼rkÃ§e gibi sondan eklemeli dillerde, kelime kÃ¶kleri ve ekler arasÄ±ndaki iliÅŸkiyi modellemek Ã¶nemlidir. 
        Ã–rneÄŸin, "kitaplarÄ±mÄ±zdakiler" kelimesi "kitap-lar-Ä±mÄ±z-da-ki-ler" ÅŸeklinde morfolojik birimlerine ayrÄ±labilir. 
        Bu tÃ¼r dillerde alt kelime tabanlÄ± tokenizasyon yÃ¶ntemleri (BPE, WordPiece, Unigram vb.) daha etkili sonuÃ§lar verebilir.
        
        Tokenizasyon algoritmalarÄ±, bÃ¼yÃ¼k Ã¶lÃ§ekli veri kÃ¼melerinde eÄŸitilir ve sÃ¶zlÃ¼k boyutuna gÃ¶re optimize edilir. 
        SÃ¶zlÃ¼k boyutunun artmasÄ±, daha spesifik tokenlerin Ã¶ÄŸrenilmesine olanak tanÄ±r ancak modelin boyutu ve iÅŸlem sÃ¼resi de artar.
        
        TÃ¼rkÃ§e'nin morfolojik yapÄ±sÄ± nedeniyle, etkili bir tokenizasyon stratejisi geliÅŸtirmek Ã¶nemlidir. 
        Aksi takdirde, seyrek gÃ¶rÃ¼len kelimeler ve uzun bileÅŸik yapÄ±lar, modelin performansÄ±nÄ± olumsuz etkileyebilir."""
    
    def _get_english_sample(self):
        """
        Ä°ngilizce metin Ã¶rneÄŸi oluÅŸturur.
        """
        return """Natural Language Processing (NLP) is a subfield of computer science and artificial intelligence concerned with the interactions between computers and human language.
        It involves enabling computers to understand, interpret, and generate human language in a valuable way.
        
        Tokenization is a fundamental preprocessing step in natural language processing. It aims to break down text data into smaller units.
        These units can be words, subwords, or characters. The choice of tokenization method depends on the structure of the language and the problem being solved.
        
        For languages like English, word-based tokenization might seem straightforward, splitting on spaces and punctuation. However, complications arise with
        contractions (e.g., "don't"), hyphenated words (e.g., "state-of-the-art"), and compound words (e.g., "database").
        
        Subword tokenization approaches such as Byte Pair Encoding (BPE), WordPiece, and Unigram have gained popularity in recent years.
        These methods balance the trade-off between vocabulary size and token frequency by learning common subword patterns from training data.
        
        BPE, for instance, starts with individual characters and iteratively merges the most frequent pairs to form new tokens.
        WordPiece is similar but uses a likelihood-based approach to determine which pairs to merge.
        Unigram uses a probabilistic model to find the most likely segmentation of words into subwords.
        
        The choice of tokenizer can significantly impact model performance, especially for tasks involving rare words or morphologically complex languages.
        Recent developments in NLP have moved towards character-level or byte-level tokenization to handle multilingual scenarios and reduce out-of-vocabulary issues."""
    
    def _get_mixed_language_sample(self):
        """
        KarÄ±ÅŸÄ±k dil iÃ§eren metin Ã¶rneÄŸi oluÅŸturur.
        """
        return """In software development, writing clean code is essential. "Temiz kod yazmak" (writing clean code in Turkish) involves several principles:
        
        1. Simplicity (Basitlik): "Keep it simple, stupid!" The code should be easy to understand.
        2. Clarity (Netlik): Variable and function names should clearly describe their purpose.
        3. DRY (Don't Repeat Yourself): "Kendini tekrarlama" - Avoid duplicating code.
        4. Testing (Test Etme): "Ä°yi test edilmiÅŸ kod, iyi koddur" (Well-tested code is good code).
        
        Consider this Python example with Turkish comments:
        
        ```python
        # Bir listedeki sayÄ±larÄ±n ortalamasÄ±nÄ± hesaplar
        def calculate_average(numbers):
            if not numbers:
                return 0  # BoÅŸ liste iÃ§in 0 dÃ¶ndÃ¼r
            total = sum(numbers)
            return total / len(numbers)
        ```
        
        Multilingual documentation is becoming more common in global projects. Documentation in both English and the local language helps diverse development teams collaborate effectively.
        
        "Ã‡ok dilli belgelendirme, kÃ¼resel projelerde daha yaygÄ±n hale geliyor. Ä°ngilizce ve yerel dildeki belgeler, farklÄ± geliÅŸtirme ekiplerinin etkili bir ÅŸekilde iÅŸbirliÄŸi yapmasÄ±na yardÄ±mcÄ± olur."
        
        Using the right tokenization strategy for multilingual text is crucial for NLP applications. A good tokenizer should handle different languages seamlessly, including characters like 'ÅŸ', 'Ã§', 'ÄŸ', 'Ã¼', 'Ã¶', 'Ä±' in Turkish."""
    
    def _get_technical_sample(self):
        """
        Teknik iÃ§erikli metin Ã¶rneÄŸi oluÅŸturur.
        """
        return """# Transformers ve Self-Attention MekanizmasÄ±

        ## 1. Transformers Mimarisi
        
        Transformers mimarisi, 2017 yÄ±lÄ±nda "Attention is All You Need" makalesiyle tanÄ±tÄ±lmÄ±ÅŸtÄ±r. Bu mimari, tekrarlayan sinir aÄŸlarÄ± (RNN) ve evriÅŸimli sinir aÄŸlarÄ±nÄ±n (CNN) sÄ±nÄ±rlÄ±lÄ±klarÄ±nÄ± aÅŸmak iÃ§in tasarlanmÄ±ÅŸtÄ±r. Temel yenilikÃ§i bileÅŸeni, self-attention mekanizmasÄ±dÄ±r.
        
        Mimari, encoder ve decoder olmak Ã¼zere iki ana bÃ¶lÃ¼mden oluÅŸur:
        
        - **Encoder**: GiriÅŸ dizisini iÅŸler ve her token iÃ§in baÄŸlamsal temsiller oluÅŸturur.
        - **Decoder**: Encoder Ã§Ä±ktÄ±sÄ±nÄ± ve Ã¶nceki Ã§Ä±ktÄ± tokenlerini kullanarak yeni tokenler Ã¼retir.
        
        ## 2. Self-Attention MekanizmasÄ±
        
        Self-attention, bir dizinin her Ã¶ÄŸesinin, dizinin diÄŸer tÃ¼m Ã¶ÄŸeleriyle iliÅŸkisini hesaplar. Matematiksel olarak:
        
        $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
        
        Burada:
        - Q (Query): Sorgu matrisi
        - K (Key): Anahtar matrisi
        - V (Value): DeÄŸer matrisi
        - d_k: AnahtarlarÄ±n boyutu
        
        ## 3. Ã‡ok BaÅŸlÄ± Dikkat (Multi-Head Attention)
        
        Ã‡ok baÅŸlÄ± dikkat, farklÄ± temsil alt uzaylarÄ±ndan bilgileri yakalamak iÃ§in birden fazla self-attention katmanÄ±nÄ± paralel olarak Ã§alÄ±ÅŸtÄ±rÄ±r:
        
        $$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$
        
        $$\text{where head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$
        
        ## 4. Pozisyon Kodlama (Positional Encoding)
        
        Transformers, dizilerdeki sÄ±ra bilgisini kodlamak iÃ§in pozisyon kodlamasÄ± kullanÄ±r:
        
        $$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
        $$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$
        
        ## 5. Uygulamalar
        
        Transformer mimarisi, Ã§eÅŸitli doÄŸal dil iÅŸleme gÃ¶revlerinde kullanÄ±lÄ±r:
        
        - Metin sÄ±nÄ±flandÄ±rma
        - Dil Ã§evirisi
        - Soru cevaplama
        - Metin Ã¶zetleme
        - Dil modelleme (GPT, BERT, T5)
        
        ## 6. Ä°leri Teknolojiler
        
        En son transformer tabanlÄ± modeller ÅŸunlarÄ± iÃ§erir:
        
        - BERT (Bidirectional Encoder Representations from Transformers)
        - GPT (Generative Pre-trained Transformer)
        - T5 (Text-to-Text Transfer Transformer)
        - XLNet, RoBERTa, ALBERT, ELECTRA
        
        ## 7. Verimlilik Ä°yileÅŸtirmeleri
        
        BÃ¼yÃ¼k transformerlarÄ±n eÄŸitimini ve Ã§Ä±karÄ±mÄ±nÄ± iyileÅŸtirmek iÃ§in Ã§eÅŸitli teknikler geliÅŸtirilmiÅŸtir:
        
        - Sparse Attention
        - Gradient Checkpointing
        - Mixed Precision Training
        - Model Parallelism
        - Pipeline Parallelism
        """
    
    def _get_social_media_sample(self):
        """
        Sosyal medya tarzÄ± metin Ã¶rneÄŸi oluÅŸturur.
        """
        return """BugÃ¼n #AITokenizer projemizi aÃ§Ä±k kaynak olarak paylaÅŸtÄ±k! ğŸ‰ @TurkishNLP ekibine bÃ¼yÃ¼k teÅŸekkÃ¼rler! ğŸ‘

        Bu tokenizer farklÄ± dilleri destekliyor:
        - TÃ¼rkÃ§e ğŸ‡¹ğŸ‡·
        - Ä°ngilizce ğŸ‡¬ğŸ‡§
        - Almanca ğŸ‡©ğŸ‡ª
        
        Projemiz hakkÄ±nda daha fazla bilgiyi ğŸ‘‰ https://github.com/example/ai-tokenizer adresinden bulabilirsiniz.
        
        #NLP #MachineLearning #OpenSource
        
        ---
        
        @user1 hÄ±zlÄ± bir soru - 5B parametreli modeli 4xA100 GPU'da nasÄ±l eÄŸitebilirim? Bellek hatasÄ± alÄ±yorum ğŸ˜«
        
        @user2 DeepSpeed ya da FSDP kullan, gradient checkpointing aÃ§mayÄ± unutma! ben Ã¶nceki projemde ZeRO-3 kullandÄ±m, GPU baÅŸÄ±na 20GB bellek tÃ¼ketimim vardÄ±.
        
        @user3 Proje iÃ§in PR yolladÄ±m, inceleyebilir misin? Unicode normalization sorununu Ã§Ã¶zdÃ¼m.
        
        OMG! Yeni model 42% daha iyi Ã§alÄ±ÅŸÄ±yor! SOTA sonuÃ§larÄ± geÃ§tik! ğŸš€ğŸ”¥
        
        ---
        
        Bu gÃ¶nderide bugÃ¼n iÃ§in to-do listemi paylaÅŸÄ±yorum:
        âœ… Tokenizer eÄŸitimi (3 saat sÃ¼rdÃ¼...)
        âœ… Hata analizi
        âŒ DokÃ¼manlarÄ± gÃ¼ncelleme
        âŒ Test coverage artÄ±rma
        
        En sevdiÄŸim meme: "bir tokenizer Ã¶ÄŸrendiÄŸinde, aslÄ±nda Ã¶ÄŸrendiÄŸin dildir" ğŸ¤£
        
        RT @ML_News: 2023 yÄ±lÄ±nda tokenizer'lar Ã¼zerine yayÄ±nlanan 10 Ã¶nemli Ã§alÄ±ÅŸma: [LINK]
        
        /cc @researcher1 @researcher2
        """
    
    def test_individual_tokenizers_on_languages(self):
        """
        Her tokenizer'Ä± farklÄ± doÄŸal dil Ã¶rnekleri Ã¼zerinde test eder.
        """
        # Test edilecek tokenizer'lar
        tokenizer_instances = {
            "BPE": self.BPETokenizer(vocab_size=1000),
            "WordPiece": self.WordPieceTokenizer(vocab_size=1000),
            "ByteLevel": self.ByteLevelTokenizer(vocab_size=1000),
            "Unigram": self.UnigramTokenizer(vocab_size=1000)
        }
        
        # TÃ¼m dil Ã¶rneklerini birleÅŸtir ve eÄŸitim iÃ§in kullan
        all_samples = list(self.language_samples.values())
        
        # Her tokenizer'Ä± test et
        for tokenizer_name, tokenizer in tokenizer_instances.items():
            with self.subTest(tokenizer=tokenizer_name):
                # Tokenizer'Ä± eÄŸit
                print(f"\n{tokenizer_name} tokenizer doÄŸal dil Ã¼zerinde eÄŸitiliyor...")
                train_result = tokenizer.train(all_samples)
                
                self.assertTrue(tokenizer.is_trained, f"{tokenizer_name} eÄŸitimi baÅŸarÄ±sÄ±z oldu")
                self.assertGreater(tokenizer.get_vocab_size(), 0, f"{tokenizer_name} boÅŸ sÃ¶zlÃ¼k oluÅŸturdu")
                
                # FarklÄ± dil Ã¶rneklerini test et
                for lang_name, text in self.language_samples.items():
                    sample_text = text[:500]  # Ä°lk 500 karakteri test et
                    
                    # Encode ve decode iÅŸlemleri
                    encoded = tokenizer.encode(sample_text)
                    decoded = tokenizer.decode(encoded)
                    
                    # SonuÃ§larÄ± yazdÄ±r
                    print(f"{tokenizer_name} - {lang_name} encode sonucu: {len(encoded)} token")
                    print(f"Ä°lk 10 token ID: {encoded[:10]}")
                    
                    # Token yoÄŸunluÄŸunu hesapla (token sayÄ±sÄ± / metin uzunluÄŸu)
                    density = len(encoded) / len(sample_text)
                    print(f"Token yoÄŸunluÄŸu: {density:.4f} token/karakter")
                    
                    # Benzerlik skoru hesapla
                    similarity = self._text_similarity(sample_text, decoded)
                    print(f"Decode benzerliÄŸi: {similarity:.2f}%")
                    
                    # Minimal doÄŸrulama
                    self.assertGreater(len(encoded), 0, f"{tokenizer_name} hiÃ§ token Ã¼retmedi")
                
                # Tokenizer'Ä± kaydet ve yÃ¼kle
                save_path = self.temp_path / f"{tokenizer_name}_nlp"
                tokenizer.save(save_path)
                
                try:
                    loaded_tokenizer = tokenizer.__class__.load(save_path)
                    self.assertEqual(
                        tokenizer.get_vocab_size(), 
                        loaded_tokenizer.get_vocab_size(), 
                        f"{tokenizer_name} yÃ¼kleme sonrasÄ± sÃ¶zlÃ¼k boyutu deÄŸiÅŸti"
                    )
                except Exception as e:
                    print(f"{tokenizer_name} yÃ¼klenirken hata oluÅŸtu: {e}")
    
    def test_multilingual_tokenizer(self):
        """
        MixedTokenizer'Ä± farklÄ± diller iÃ§in test eder.
        """
        # Alt tokenizer'larÄ± oluÅŸtur
        tr_tokenizer = self.BPETokenizer(vocab_size=500, name="TurkishTokenizer")
        en_tokenizer = self.BPETokenizer(vocab_size=500, name="EnglishTokenizer")
        tech_tokenizer = self.WordPieceTokenizer(vocab_size=300, name="TechnicalTokenizer")
        social_tokenizer = self.ByteLevelTokenizer(vocab_size=300, name="SocialTokenizer")
        
        # Ã–rnekleri ayrÄ± ayrÄ± eÄŸit
        tr_tokenizer.train([self.language_samples["turkish"]])
        en_tokenizer.train([self.language_samples["english"]])
        tech_tokenizer.train([self.language_samples["technical"]])
        social_tokenizer.train([self.language_samples["social_media"]])
        
        # MixedTokenizer oluÅŸtur
        mixed_tokenizer = self.MixedTokenizer(
            tokenizers={
                "tr": tr_tokenizer,
                "en": en_tokenizer, 
                "tech": tech_tokenizer,
                "social": social_tokenizer
            },
            default_tokenizer="tr",
            merged_vocab=True,
            name="MultilingualMixed"
        )
        
        # Router fonksiyonu tanÄ±mla
        def router(text):
            # Basit bir dil/iÃ§erik tespiti
            tr_chars = sum(1 for c in text if c in "ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅÄ°Ã–Ã‡")
            hashtags = text.count("#")
            mentions = text.count("@")
            equations = sum(1 for c in text if c in "âˆ«âˆ‘âˆâˆšâˆ‚âˆ‡âˆ†âˆ‰âˆˆ")
            
            if hashtags > 0 or mentions > 0:
                return "social"
            elif equations > 0 or "```" in text:
                return "tech"
            elif tr_chars > 10:
                return "tr"
            else:
                return "en"
        
        mixed_tokenizer.router = router
        
        # Test et
        for lang_name, text in self.language_samples.items():
            sample_text = text[:300]  # Ä°lk 300 karakteri test et
            
            # Encode ve decode iÅŸlemleri
            encoded = mixed_tokenizer.encode(sample_text)
            decoded = mixed_tokenizer.decode(encoded)
            
            # SonuÃ§larÄ± yazdÄ±r
            print(f"\nMixedTokenizer - {lang_name} encode sonucu: {len(encoded)} token")
            print(f"Ä°lk 10 token ID: {encoded[:10]}")
            
            # Token yoÄŸunluÄŸunu hesapla
            density = len(encoded) / len(sample_text)
            print(f"Token yoÄŸunluÄŸu: {density:.4f} token/karakter")
            
            # Benzerlik skoru hesapla
            similarity = self._text_similarity(sample_text, decoded)
            print(f"Decode benzerliÄŸi: {similarity:.2f}%")
            
            # Dil tespitini kontrol et
            detected = router(sample_text)
            print(f"Tespit edilen tokenizer: {detected}")
            
            # Minimal doÄŸrulama
            self.assertGreater(len(encoded), 0, f"MixedTokenizer {lang_name} iÃ§in hiÃ§ token Ã¼retmedi")
        
        # Kaydet ve yÃ¼kle
        save_path = self.temp_path / "mixed_multilingual"
        mixed_tokenizer.save(save_path)
        
        try:
            loaded_tokenizer = self.MixedTokenizer.load(save_path)
            self.assertEqual(
                mixed_tokenizer.get_vocab_size(), 
                loaded_tokenizer.get_vocab_size(), 
                "MixedTokenizer yÃ¼kleme sonrasÄ± sÃ¶zlÃ¼k boyutu deÄŸiÅŸti"
            )
        except Exception as e:
            print(f"MixedTokenizer yÃ¼klenirken hata oluÅŸtu: {e}")
    
    def test_special_chars_and_emojis(self):
        """
        Ã–zel karakterleri ve emojileri test eder.
        """
        # Ã–zel karakterler ve emojiler iÃ§eren metin
        special_text = """
        Emojiler: ğŸ˜€ ğŸ˜ƒ ğŸ˜„ ğŸ˜ ğŸ˜† ğŸ˜… ğŸ˜‚ ğŸ¤£ ğŸ˜Š ğŸ˜‡ ğŸ™‚ ğŸ™ƒ ğŸ˜‰ ğŸ˜Œ ğŸ˜ ğŸ¥° ğŸ˜˜
        Matematiksel semboller: âˆ«âˆ‘âˆâˆšâˆ‚âˆ‡âˆ†âˆ‰âˆˆ â‰¤â‰¥Â±Ã—Ã·â„â„¤â„šâ„•â„‚
        MÃ¼zik sembolleri: â™©â™ªâ™«â™¬â™­â™®â™¯
        Ok iÅŸaretleri: â†â†’â†‘â†“â†”â†•â†–â†—â†˜â†™
        Kutu Ã§izim: â”Œâ”¬â”â”œâ”¼â”¤â””â”´â”˜â”‚â”€
        Kalp ve diÄŸer semboller: â¤ï¸ğŸ’•ğŸ’”ğŸ’– â­âœ¨âš¡ï¸ âœ…âŒâ›”
        TakÄ±m sembolleri: â™ ï¸â™¥ï¸â™¦ï¸â™£ï¸
        Telefon sembolleri: â˜ï¸ğŸ“±ğŸ“²ğŸ“ğŸ“ŸğŸ“ 
        Standart semboller: Â©Â®â„¢Â§Â¶â€ â€¡
        Para birimleri: $â‚¬Â£Â¥â‚ºâ‚½â‚¹
        """
        
        # ByteLevel tokenizer kullan (emoji ve Ã¶zel karakterler iÃ§in en uygun)
        tokenizer = self.ByteLevelTokenizer(vocab_size=500)
        tokenizer.train([special_text])
        
        # Encode ve decode
        encoded = tokenizer.encode(special_text)
        decoded = tokenizer.decode(encoded)
        
        # SonuÃ§larÄ± yazdÄ±r
        print("\nÃ–zel karakterler ve emojiler testi:")
        print(f"Encode sonucu: {len(encoded)} token")
        print(f"Ä°lk 20 token ID: {encoded[:20]}")
        
        # BazÄ± Ã¶zel karakterlerin doÄŸru ÅŸekilde decode edildiÄŸini kontrol et
        for char_set in ["ğŸ˜€", "âˆ«", "â™«", "â†’", "â¤ï¸", "â‚¬"]:
            self.assertIn(char_set, decoded, f"'{char_set}' karakteri decode edilmedi")
    
    def _text_similarity(self, original: str, decoded: str) -> float:
        """
        Ä°ki metin arasÄ±ndaki benzerliÄŸi hesaplar.
        
        Args:
            original: Orijinal metin
            decoded: Decode edilmiÅŸ metin
            
        Returns:
            float: Benzerlik yÃ¼zdesi (0-100)
        """
        # BasitleÅŸtirilmiÅŸ benzerlik: boÅŸluklarÄ± temizle ve karakterleri karÅŸÄ±laÅŸtÄ±r
        original_clean = ''.join(original.split())
        decoded_clean = ''.join(decoded.split())
        
        # Minimum uzunluk Ã¼zerinden karakterleri karÅŸÄ±laÅŸtÄ±r
        min_len = min(len(original_clean), len(decoded_clean))
        if min_len == 0:
            return 0.0
            
        matches = sum(1 for i in range(min_len) if original_clean[i] == decoded_clean[i])
        return (matches / min_len) * 100


if __name__ == "__main__":
    unittest.main()